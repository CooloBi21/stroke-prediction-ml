"""
Module Evaluation Models - Stroke Prediction
Confusion Matrix, ROC Curve, Precision-Recall Curve, Feature Importance
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve,
    accuracy_score, precision_score, recall_score, f1_score
)
import joblib
import os
import warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

class ModelEvaluator:
    """Class ƒë√°nh gi√° hi·ªáu su·∫•t models"""
    
    def __init__(self, config, models_dir='models'):
        self.config = config
        self.models_dir = models_dir
        self.models = {}
        self.evaluation_results = {}
    
    def load_models(self):
        """Load t·∫•t c·∫£ models ƒë√£ train"""
        print("\n" + "="*60)
        print("üì• ƒêANG LOAD MODELS...")
        print("="*60)
        
        model_files = [f for f in os.listdir(self.models_dir)
                      if f.endswith('.pkl') and f != 'cv_results.pkl' 
                      and f != 'preprocessor.pkl']
        
        for model_file in model_files:
            model_name = model_file.replace('.pkl', '')
            filepath = os.path.join(self.models_dir, model_file)
            self.models[model_name] = joblib.load(filepath)
            print(f"‚úì ƒê√£ load {model_name}")
        
        print(f"\n‚úì T·ªïng c·ªông: {len(self.models)} models")
    
    def evaluate_single_model(self, model, X_test, y_test, model_name):
        """ƒê√°nh gi√° m·ªôt model tr√™n test set"""
        print(f"\n{'='*60}")
        print(f"üîç ƒê√ÅNH GI√Å {model_name.upper()}")
        print(f"{'='*60}")
        
        # Predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # T√≠nh c√°c metrics
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred)
        }
        
        # ROC AUC
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        metrics['roc_auc'] = auc(fpr, tpr)
        
        # Print metrics
        print(f"\nüìä Metrics:")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f} ‚Üê T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng trong c√°c ca ƒêO√ÅN l√† ƒë·ªôt qu·ªµ")
        print(f"  Recall: {metrics['recall']:.4f} ‚Üê T·ª∑ l·ªá ph√°t hi·ªán ƒë∆∞·ª£c ca ƒë·ªôt qu·ªµ TH·ª∞C S·ª∞ (quan tr·ªçng nh·∫•t!)")
        print(f"  F1-Score: {metrics['f1_score']:.4f} ‚Üê C√¢n b·∫±ng Precision v√† Recall")
        print(f"  ROC AUC: {metrics['roc_auc']:.4f}")
        
        # Ph√¢n t√≠ch y t·∫ø
        print(f"\nüè• Ph√¢n t√≠ch y t·∫ø:")
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        print(f"  True Positives (TP): {tp} - Ph√°t hi·ªán ƒê√öNG ca ƒë·ªôt qu·ªµ")
        print(f"  False Negatives (FN): {fn} - B·ªé S√ìT ca ƒë·ªôt qu·ªµ (nguy hi·ªÉm!)")
        print(f"  False Positives (FP): {fp} - C·∫£nh b√°o SAI (d∆∞∆°ng t√≠nh gi·∫£)")
        print(f"  True Negatives (TN): {tn} - Ph√°t hi·ªán ƒë√∫ng ca KH√îNG ƒë·ªôt qu·ªµ")
        
        if fn > 0:
            print(f"\n  ‚ö†Ô∏è C·∫¢NH B√ÅO: C√≥ {fn} ca ƒë·ªôt qu·ªµ B·ªä B·ªé S√ìT!")
            print(f"    ‚Üí Trong y t·∫ø, False Negative r·∫•t nguy hi·ªÉm!")
        
        # L∆∞u k·∫øt qu·∫£
        self.evaluation_results[model_name] = {
            'metrics': metrics,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'confusion_matrix': confusion_matrix(y_test, y_pred)
        }
        
        return metrics
    
    def plot_confusion_matrices(self, y_test, figsize=(16, 4)):
        """V·∫Ω confusion matrices cho t·∫•t c·∫£ models"""
        print(f"\n{'='*60}")
        print("üìä T·∫†O CONFUSION MATRICES")
        print(f"{'='*60}")
        
        n_models = len(self.evaluation_results)
        fig, axes = plt.subplots(1, n_models, figsize=figsize)
        
        if n_models == 1:
            axes = [axes]
        
        for idx, (model_name, results) in enumerate(self.evaluation_results.items()):
            cm = results['confusion_matrix']
            
            sns.heatmap(
                cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Stroke', 'Stroke'],
                yticklabels=['No Stroke', 'Stroke'],
                ax=axes[idx],
                cbar=False
            )
            
            axes[idx].set_title(f'{model_name}\nConfusion Matrix',
                              fontweight='bold', fontsize=12)
            axes[idx].set_ylabel('Actual', fontsize=10)
            axes[idx].set_xlabel('Predicted', fontsize=10)
            
            # Th√™m annotations cho y t·∫ø
            tn, fp, fn, tp = cm.ravel()
            axes[idx].text(0.5, -0.2,
                         f'FN={fn} (B·ªè s√≥t)',
                         transform=axes[idx].transAxes,
                         ha='center', fontsize=9, color='red')
        
        plt.tight_layout()
        plt.savefig('results/confusion_matrices.png', dpi=300, bbox_inches='tight')
        print("‚úì ƒê√£ l∆∞u: results/confusion_matrices.png")
        plt.show()
    
    def plot_roc_curves(self, y_test, figsize=(10, 8)):
        """V·∫Ω ROC curves cho t·∫•t c·∫£ models"""
        print(f"\n{'='*60}")
        print("üìà T·∫†O ROC CURVES")
        print(f"{'='*60}")
        
        plt.figure(figsize=figsize)
        
        for model_name, results in self.evaluation_results.items():
            y_pred_proba = results['y_pred_proba']
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            
            plt.plot(fpr, tpr, linewidth=2,
                    label=f'{model_name} (AUC = {roc_auc:.4f})')
        
        # ƒê∆∞·ªùng baseline
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
        plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)
        plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/roc_curves.png', dpi=300, bbox_inches='tight')
        print("‚úì ƒê√£ l∆∞u: results/roc_curves.png")
        plt.show()
    
    def plot_precision_recall_curves(self, y_test, figsize=(10, 8)):
        """V·∫Ω Precision-Recall curves (quan tr·ªçng cho imbalanced data)"""
        print(f"\n{'='*60}")
        print("üìà T·∫†O PRECISION-RECALL CURVES")
        print(f"{'='*60}")
        
        plt.figure(figsize=figsize)
        
        for model_name, results in self.evaluation_results.items():
            y_pred_proba = results['y_pred_proba']
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            pr_auc = auc(recall, precision)
            
            plt.plot(recall, precision, linewidth=2,
                    label=f'{model_name} (AUC = {pr_auc:.4f})')
        
        # Baseline
        baseline = (y_test == 1).sum() / len(y_test)
        plt.plot([0, 1], [baseline, baseline], 'k--', linewidth=1,
                label=f'Baseline ({baseline:.4f})')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall (Sensitivity)', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curves - Model Comparison\n(Quan tr·ªçng cho Imbalanced Data)',
                 fontsize=14, fontweight='bold')
        plt.legend(loc="best", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/precision_recall_curves.png', dpi=300, bbox_inches='tight')
        print("‚úì ƒê√£ l∆∞u: results/precision_recall_curves.png")
        plt.show()
    
    def plot_feature_importance(self, X_test, top_n=15, figsize=(12, 8)):
        """V·∫Ω feature importance cho tree-based models"""
        print(f"\n{'='*60}")
        print("üìä PH√ÇN T√çCH FEATURE IMPORTANCE")
        print(f"{'='*60}")
        
        tree_based_models = ['RandomForest', 'XGBoost', 'LightGBM']
        available_models = [m for m in tree_based_models if m in self.models]
        
        if not available_models:
            print("‚ö†Ô∏è Kh√¥ng c√≥ tree-based model n√†o!")
            return
        
        n_models = len(available_models)
        fig, axes = plt.subplots(1, n_models, figsize=figsize)
        
        if n_models == 1:
            axes = [axes]
        
        for idx, model_name in enumerate(available_models):
            model = self.models[model_name]
            
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
            else:
                continue
            
            # T·∫°o DataFrame
            feature_importance_df = pd.DataFrame({
                'feature': X_test.columns,
                'importance': importances
            }).sort_values('importance', ascending=False).head(top_n)
            
            # Plot
            axes[idx].barh(range(len(feature_importance_df)),
                         feature_importance_df['importance'],
                         color='steelblue')
            axes[idx].set_yticks(range(len(feature_importance_df)))
            axes[idx].set_yticklabels(feature_importance_df['feature'])
            axes[idx].set_xlabel('Importance', fontsize=10)
            axes[idx].set_title(f'{model_name}\nTop {top_n} Features',
                              fontweight='bold', fontsize=12)
            axes[idx].invert_yaxis()
            axes[idx].grid(axis='x', alpha=0.3)
            
            # In ra console
            print(f"\nüîù Top 5 features c·ªßa {model_name}:")
            for i, row in feature_importance_df.head(5).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
        
        plt.tight_layout()
        plt.savefig('results/feature_importance.png', dpi=300, bbox_inches='tight')
        print("\n‚úì ƒê√£ l∆∞u: results/feature_importance.png")
        plt.show()
    
    def generate_classification_reports(self, y_test):
        """T·∫°o classification reports chi ti·∫øt"""
        print(f"\n{'='*60}")
        print("üìã CLASSIFICATION REPORTS")
        print(f"{'='*60}")
        
        for model_name, results in self.evaluation_results.items():
            y_pred = results['y_pred']
            
            print(f"\n{'‚îÄ'*60}")
            print(f"{model_name}")
            print(f"{'‚îÄ'*60}")
            print(classification_report(y_test, y_pred,
                                       target_names=['No Stroke', 'Stroke'],
                                       digits=4))
    
    def create_comparison_table(self):
        """T·∫°o b·∫£ng so s√°nh c√°c models"""
        print(f"\n{'='*60}")
        print("üìä B·∫¢NG SO S√ÅNH MODELS")
        print(f"{'='*60}")
        
        comparison_data = []
        for model_name, results in self.evaluation_results.items():
            metrics = results['metrics']
            comparison_data.append({
                'Model': model_name,
                'Accuracy': f"{metrics['accuracy']:.4f}",
                'Precision': f"{metrics['precision']:.4f}",
                'Recall': f"{metrics['recall']:.4f}",
                'F1-Score': f"{metrics['f1_score']:.4f}",
                'ROC AUC': f"{metrics['roc_auc']:.4f}"
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        print(f"\n{df_comparison.to_string(index=False)}")
        
        # L∆∞u ra CSV
        df_comparison.to_csv('results/model_comparison.csv', index=False)
        print("\n‚úì ƒê√£ l∆∞u: results/model_comparison.csv")
        
        # T√¨m best model
        best_recall_idx = df_comparison['Recall'].astype(float).idxmax()
        
        print(f"\nüèÜ BEST MODEL (theo Recall - quan tr·ªçng nh·∫•t):")
        print(f"  {df_comparison.loc[best_recall_idx, 'Model']}")
        print(f"  Recall: {df_comparison.loc[best_recall_idx, 'Recall']}")
        
        return df_comparison

def main():
    """Demo evaluation pipeline"""
    import yaml
    
    print("\n" + "üéØ "*20)
    print("MODEL EVALUATION MODULE")
    print("üéØ "*20)
    
    # Load config
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # T·∫°o th∆∞ m·ª•c results
    os.makedirs('results', exist_ok=True)
    
    # Load test data
    X_test = pd.read_csv('data/processed/X_test.csv')
    y_test = pd.read_csv('data/processed/y_test.csv').values.ravel()
    
    print(f"\nüìä Test data shape: {X_test.shape}")
    print(f"üìä Class distribution: {np.bincount(y_test)}")
    
    # Kh·ªüi t·∫°o evaluator
    evaluator = ModelEvaluator(config, models_dir='models')
    
    # Load models
    evaluator.load_models()
    
    # Evaluate t·ª´ng model
    print("\n" + "üîç "*20)
    print("B·∫ÆT ƒê·∫¶U EVALUATION")
    print("üîç "*20)
    
    for model_name, model in evaluator.models.items():
        evaluator.evaluate_single_model(model, X_test, y_test, model_name)
    
    # Visualizations
    if config['evaluation']['visualizations']:
        print("\n" + "üìä "*20)
        print("T·∫†O VISUALIZATIONS")
        print("üìä "*20)
        
        evaluator.plot_confusion_matrices(y_test)
        evaluator.plot_roc_curves(y_test)
        evaluator.plot_precision_recall_curves(y_test)
        evaluator.plot_feature_importance(X_test)
    
    # Reports
    evaluator.generate_classification_reports(y_test)
    df_comparison = evaluator.create_comparison_table()
    
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN TH√ÄNH EVALUATION!")
    print("="*60)
    print("\nüìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u trong th∆∞ m·ª•c 'results/'")

if __name__ == "__main__":
    main()