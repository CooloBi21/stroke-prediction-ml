"""
Module Preprocessing d·ªØ li·ªáu y t·∫ø - Stroke Prediction
X·ª≠ l√Ω missing values, encoding, scaling
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import KNNImputer
import joblib
import os

class StrokeDataPreprocessor:
    """Class x·ª≠ l√Ω d·ªØ li·ªáu b·ªánh nh√¢n ƒë·ªôt qu·ªµ"""
    
    def __init__(self, config):
        self.config = config
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.knn_imputer = KNNImputer(n_neighbors=config['preprocessing']['knn_neighbors'])
        
    def load_data(self, filepath):
        """Load d·ªØ li·ªáu t·ª´ CSV"""
        print("="*60)
        print("üìÇ ƒêANG LOAD D·ªÆ LI·ªÜU...")
        print("="*60)
        
        df = pd.read_csv(filepath)
        print(f"\n‚úì S·ªë l∆∞·ª£ng b·ªánh nh√¢n: {len(df)}")
        print(f"‚úì S·ªë l∆∞·ª£ng features: {df.shape[1]}")
        
        # Ph√¢n t√≠ch class imbalance
        stroke_counts = df['stroke'].value_counts()
        print(f"\n‚ö†Ô∏è PH√ÇN T√çCH M·∫§T C√ÇN B·∫∞NG D·ªÆ LI·ªÜU:")
        print(f"  - Kh√¥ng ƒë·ªôt qu·ªµ (0): {stroke_counts[0]} ({stroke_counts[0]/len(df)*100:.2f}%)")
        print(f"  - C√≥ ƒë·ªôt qu·ªµ (1): {stroke_counts[1]} ({stroke_counts[1]/len(df)*100:.2f}%)")
        print(f"  - T·ª∑ l·ªá imbalance: 1:{stroke_counts[0]/stroke_counts[1]:.1f}")
        
        return df
    
    def handle_missing_values(self, df):
        """X·ª≠ l√Ω missing values b·∫±ng KNN Imputer"""
        print("\n" + "="*60)
        print("üîß X·ª¨ L√ù MISSING VALUES...")
        print("="*60)
        
        missing = df.isnull().sum()
        if missing.sum() > 0:
            print(f"\nüìã Missing values tr∆∞·ªõc khi x·ª≠ l√Ω:")
            for col in missing[missing > 0].index:
                print(f"  - {col}: {missing[col]} ({missing[col]/len(df)*100:.2f}%)")
        
        # X·ª≠ l√Ω BMI b·∫±ng KNN Imputer
        if df['bmi'].isnull().sum() > 0:
            print(f"\nüîÑ ƒêang impute BMI b·∫±ng KNN (k={self.config['preprocessing']['knn_neighbors']})...")
            numeric_cols = ['age', 'avg_glucose_level', 'bmi']
            df[numeric_cols] = self.knn_imputer.fit_transform(df[numeric_cols])
            print(f"‚úì ƒê√£ impute {missing['bmi']} gi√° tr·ªã BMI")
        
        # X·ª≠ l√Ω smoking_status n·∫øu c√≥ missing
        if 'smoking_status' in df.columns and df['smoking_status'].isnull().sum() > 0:
            df['smoking_status'].fillna('Unknown', inplace=True)
        
        print(f"\n‚úì Ho√†n th√†nh! T·ªïng missing values c√≤n l·∫°i: {df.isnull().sum().sum()}")
        return df
    
    def encode_categorical_features(self, df):
        """Encoding c√°c features categorical"""
        print("\n" + "="*60)
        print("üè∑Ô∏è ENCODING CATEGORICAL FEATURES...")
        print("="*60)
        
        df_encoded = df.copy()
        categorical_cols = ['gender', 'ever_married', 'work_type', 
                          'Residence_type', 'smoking_status']
        
        for col in categorical_cols:
            if col in df_encoded.columns:
                print(f"\n  Encoding {col}: {df_encoded[col].unique()}")
                self.label_encoders[col] = LabelEncoder()
                df_encoded[col] = self.label_encoders[col].fit_transform(df_encoded[col])
        
        print(f"\n‚úì ƒê√£ encode {len(categorical_cols)} features")
        return df_encoded
    
    def prepare_data(self, df):
        """Chu·∫©n b·ªã d·ªØ li·ªáu: train-test split v√† scaling"""
        print("\n" + "="*60)
        print("‚úÇÔ∏è CHIA D·ªÆ LI·ªÜU TRAIN-TEST...")
        print("="*60)
        
        # Lo·∫°i b·ªè c·ªôt id
        if 'id' in df.columns:
            df = df.drop('id', axis=1)
        
        # T√°ch features v√† target
        X = df.drop('stroke', axis=1)
        y = df['stroke']
        
        # Stratified split (quan tr·ªçng cho imbalanced data!)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=self.config['data']['test_size'],
            random_state=self.config['data']['random_state'],
            stratify=y
        )
        
        print(f"\nüìä K·∫øt qu·∫£ chia d·ªØ li·ªáu:")
        print(f"  - Training set: {len(X_train)} samples")
        print(f"  - Testing set: {len(X_test)} samples")
        print(f"\n  Ph√¢n b·ªë class trong training set:")
        print(f"  - Class 0: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)")
        print(f"  - Class 1: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)")
        
        # Scaling features
        print(f"\nüìè CHU·∫®N H√ìA FEATURES...")
        scale_cols = self.config['preprocessing']['scale_features']
        print(f"  Scaling columns: {scale_cols}")
        
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()
        
        X_train_scaled[scale_cols] = self.scaler.fit_transform(X_train[scale_cols])
        X_test_scaled[scale_cols] = self.scaler.transform(X_test[scale_cols])
        
        print(f"‚úì ƒê√£ scale training v√† testing set")
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def save_preprocessor(self, filepath='models/preprocessor.pkl'):
        """L∆∞u preprocessor ƒë·ªÉ s·ª≠ d·ª•ng sau"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        preprocessor_data = {
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'knn_imputer': self.knn_imputer
        }
        joblib.dump(preprocessor_data, filepath)
        print(f"\nüíæ ƒê√£ l∆∞u preprocessor t·∫°i: {filepath}")

def main():
    """Demo preprocessing pipeline"""
    import yaml
    
    print("\n" + "üè• "*20)
    print("STROKE PREDICTION - DATA PREPROCESSING")
    print("üè• "*20)
    
    # Load config
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # Kh·ªüi t·∫°o preprocessor
    preprocessor = StrokeDataPreprocessor(config)
    
    # Load d·ªØ li·ªáu
    df = preprocessor.load_data(config['data']['raw_path'])
    
    # X·ª≠ l√Ω missing values
    df = preprocessor.handle_missing_values(df)
    
    # Encoding categorical
    df = preprocessor.encode_categorical_features(df)
    
    # Chia train-test v√† scale
    X_train, X_test, y_train, y_test = preprocessor.prepare_data(df)
    
    # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
    os.makedirs('data/processed', exist_ok=True)
    X_train.to_csv('data/processed/X_train.csv', index=False)
    X_test.to_csv('data/processed/X_test.csv', index=False)
    y_train.to_csv('data/processed/y_train.csv', index=False)
    y_test.to_csv('data/processed/y_test.csv', index=False)
    
    print("\nüíæ ƒê√£ l∆∞u d·ªØ li·ªáu processed:")
    print("  - data/processed/X_train.csv")
    print("  - data/processed/X_test.csv")
    print("  - data/processed/y_train.csv")
    print("  - data/processed/y_test.csv")
    
    # L∆∞u preprocessor
    preprocessor.save_preprocessor()
    
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN TH√ÄNH PREPROCESSING!")
    print("="*60)

if __name__ == "__main__":
    main()