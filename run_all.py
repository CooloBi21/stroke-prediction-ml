"""
Script ch·∫°y to√†n b·ªô pipeline ML
T·ª´ preprocessing ‚Üí imbalanced handling ‚Üí training ‚Üí evaluation
"""
import os
import sys
import yaml
import time
from datetime import datetime

# Th√™m src v√†o path
sys.path.append('src')

from data_preprocessing import StrokeDataPreprocessor
from imbalanced_handler import ImbalancedDataHandler
from model_training import StrokeModelTrainer
from model_evaluation import ModelEvaluator

def create_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    directories = [
        'data/raw',
        'data/processed',
        'data/processed/resampled',
        'models',
        'results'
    ]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

def print_header(text):
    """In header ƒë·∫πp"""
    print("\n" + "="*80)
    print(f"  {text}")
    print("="*80)

def print_step(step_num, step_name):
    """In b∆∞·ªõc th·ª±c hi·ªán"""
    print(f"\n{'üîπ'*30}")
    print(f"  B∆Ø·ªöC {step_num}: {step_name}")
    print(f"{'üîπ'*30}\n")

def main():
    """Main pipeline execution"""
    start_time = time.time()
    
    print("\n" + "üè• "*40)
    print(" "*20 + "STROKE PREDICTION ML PIPELINE")
    print(" "*20 + f"B·∫Øt ƒë·∫ßu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("üè• "*40)
    
    # ========================================
    # SETUP
    # ========================================
    print_header("SETUP & INITIALIZATION")
    
    print("üìÅ T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt...")
    create_directories()
    
    print("‚öôÔ∏è Load c·∫•u h√¨nh...")
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # Ki·ªÉm tra dataset
    data_path = config['data']['raw_path']
    if not os.path.exists(data_path):
        print(f"\n‚ùå ERROR: Dataset kh√¥ng t√¨m th·∫•y t·∫°i {data_path}")
        print("üì• Vui l√≤ng download dataset t·ª´ Kaggle:")
        print("   https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset")
        sys.exit(1)
    
    print("‚úì Setup ho√†n t·∫•t!")
    
    # ========================================
    # STEP 1: DATA PREPROCESSING
    # ========================================
    print_step(1, "DATA PREPROCESSING")
    
    preprocessor = StrokeDataPreprocessor(config)
    
    # Load data
    df = preprocessor.load_data(data_path)
    
    # Handle missing values
    df = preprocessor.handle_missing_values(df)
    
    # Encode categorical
    df = preprocessor.encode_categorical_features(df)
    
    # Train-test split & scaling
    X_train, X_test, y_train, y_test = preprocessor.prepare_data(df)
    
    # Save processed data
    X_train.to_csv('data/processed/X_train.csv', index=False)
    X_test.to_csv('data/processed/X_test.csv', index=False)
    y_train.to_csv('data/processed/y_train.csv', index=False)
    y_test.to_csv('data/processed/y_test.csv', index=False)
    
    # Save preprocessor
    preprocessor.save_preprocessor()
    
    print("\n‚úÖ Preprocessing ho√†n t·∫•t!")
    
    # ========================================
    # STEP 2: IMBALANCED DATA HANDLING
    # ========================================
    print_step(2, "IMBALANCED DATA HANDLING")
    
    handler = ImbalancedDataHandler(config)
    
    # Ph√¢n t√≠ch imbalance
    analysis = handler.analyze_imbalance(y_train)
    
    # √Åp d·ª•ng c√°c k·ªπ thu·∫≠t resampling
    print("\nüîÑ √Åp d·ª•ng c√°c k·ªπ thu·∫≠t resampling...")
    
    X_smote, y_smote = handler.apply_smote(X_train, y_train)
    X_adasyn, y_adasyn = handler.apply_adasyn(X_train, y_train)
    X_ros, y_ros = handler.apply_random_oversampling(X_train, y_train)
    X_smote_tomek, y_smote_tomek = handler.apply_smote_tomek(X_train, y_train)
    
    # Visualizations
    handler.visualize_comparison(y_train)
    handler.print_summary()
    
    # Save resampled data (SMOTE)
    X_smote.to_csv('data/processed/resampled/X_train_smote.csv', index=False)
    y_smote.to_csv('data/processed/resampled/y_train_smote.csv', index=False)
    
    print("\n‚úÖ Imbalanced handling ho√†n t·∫•t!")
    
    # ========================================
    # STEP 3: MODEL TRAINING
    # ========================================
    print_step(3, "MODEL TRAINING")
    
    trainer = StrokeModelTrainer(config)
    
    print(f"\n‚öôÔ∏è Hyperparameter tuning: {'ENABLED' if config['training']['hyperparameter_tuning'] else 'DISABLED'}")
    print("  (Set 'hyperparameter_tuning: true' trong config.yaml ƒë·ªÉ enable)\n")
    
    # Train models
    if config['training']['models']['logistic_regression']:
        trainer.train_logistic_regression(X_smote, y_smote)
    
    if config['training']['models']['random_forest']:
        trainer.train_random_forest(X_smote, y_smote)
    
    if config['training']['models']['xgboost']:
        trainer.train_xgboost(X_smote, y_smote)
    
    if config['training']['models']['lightgbm']:
        trainer.train_lightgbm(X_smote, y_smote)
    
    # Print summary
    trainer.print_summary()
    
    # Save models
    trainer.save_models()
    
    print("\n‚úÖ Training ho√†n t·∫•t!")
    
    # ========================================
    # STEP 4: MODEL EVALUATION
    # ========================================
    print_step(4, "MODEL EVALUATION")
    
    evaluator = ModelEvaluator(config, models_dir='models')
    
    # Load models
    evaluator.load_models()
    
    # Evaluate each model
    for model_name, model in evaluator.models.items():
        evaluator.evaluate_single_model(model, X_test, y_test, model_name)
    
    # Generate visualizations
    if config['evaluation']['visualizations']:
        evaluator.plot_confusion_matrices(y_test)
        evaluator.plot_roc_curves(y_test)
        evaluator.plot_precision_recall_curves(y_test)
        evaluator.plot_feature_importance(X_test)
    
    # Classification reports
    evaluator.generate_classification_reports(y_test)
    
    # Comparison table
    df_comparison = evaluator.create_comparison_table()
    
    print("\n‚úÖ Evaluation ho√†n t·∫•t!")
    
    # ========================================
    # SUMMARY
    # ========================================
    end_time = time.time()
    total_time = end_time - start_time
    
    print_header("PIPELINE EXECUTION SUMMARY")
    
    print(f"\n‚è±Ô∏è Th·ªùi gian th·ª±c thi: {total_time:.2f}s ({total_time/60:.2f} ph√∫t)")
    
    print(f"\nüìä K·∫øt qu·∫£:")
    print(f"  - Dataset: {len(df)} b·ªánh nh√¢n")
    print(f"  - Train/Test: {len(X_train)}/{len(X_test)}")
    print(f"  - Models trained: {len(evaluator.models)}")
    print(f"  - Imbalance ratio: 1:{analysis['imbalance_ratio']:.1f}")
    
    print(f"\nüìÅ Output files:")
    print(f"  - Processed data: data/processed/")
    print(f"  - Resampled data: data/processed/resampled/")
    print(f"  - Models: models/")
    print(f"  - Results: results/")
    
    print(f"\nüèÜ Best Model:")
    best_model_name = df_comparison.loc[
        df_comparison['Recall'].astype(float).idxmax(), 'Model'
    ]
    best_recall = df_comparison.loc[
        df_comparison['Recall'].astype(float).idxmax(), 'Recall'
    ]
    print(f"  - {best_model_name} (Recall: {best_recall})")
    
    print("\n" + "="*80)
    print("‚úÖ PIPELINE HO√ÄN T·∫§T TH√ÄNH C√îNG!")
    print(f"  K·∫øt th√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    print("üí° Next steps:")
    print("  1. Xem k·∫øt qu·∫£ trong th∆∞ m·ª•c 'results/'")
    print("  2. ƒê·ªçc file 'results/model_comparison.csv'")
    print("  3. S·ª≠ d·ª•ng models trong 'models/' cho d·ª± ƒëo√°n\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Pipeline b·ªã ng·∫Øt b·ªüi ng∆∞·ªùi d√πng!")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)